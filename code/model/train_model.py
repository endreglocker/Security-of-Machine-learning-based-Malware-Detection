import torch
import torch.nn as nn  
import torch.optim as optim  
import os
import custom_dataset as cd
from custom_models import Net
import numpy as np

# calculate the accuracy of the model
def model_accuracy(model, loader):
    model.eval()  # Set the model to evaluation mode
    correct_predictions = 0
    total_predictions = 0
    with torch.no_grad():  # Disable gradient calculations
        for batch in loader:
            inputs, targets = batch
            inputs, targets = inputs.to(cd.device), targets.to(cd.device)
            # Evaluate the inputs by the model
            outputs = model(inputs)
            # Transform the ground truth values to a column vector
            targets = targets.view(-1, 1).float()
            probs = torch.sigmoid(outputs).round().float()
            # Summerize the number of correct predictions
            correct_predictions += (probs == targets).sum().item()
            # Count & Summerize the number of elements in each tensor
            total_predictions += targets.numel()

    # Calculate accuracy
    accuracy = correct_predictions / total_predictions
    return accuracy


# Model
model = Net()
model.to(cd.device)  # Move model to GPU if available

# Define the loss function and optimizer
criterion = nn.BCEWithLogitsLoss(reduction='mean')
optimizer = optim.Adam(model.parameters(), lr=cd.learning_rate, weight_decay=cd.weight_decay)

patience = 1  # Number of epochs to wait for improvement before stopping
best_loss = float('inf')  # Set default loss to infinity
epochs_without_improvement = 0  # Number of epochs without improvement

# Train the model
for epoch in range(cd.num_epochs):
    model.train()
    # X is the input (means the file as an array of bytes)
    # Y is the label (0 for benign, 1 for malware)
    i = 1
    for X, Y in cd.train_loader:
        print(f'Epoch {epoch + 1} / Batch {i}')
        i += 1
        # Move data to device
        X = X.to(device=cd.device)
        Y = Y.to(device=cd.device)

        # Forward pass
        scores = model(X)
        Y = Y.view(-1, 1).float() # Reshape Y to match the shape of scores, create a column vector
        loss = criterion(scores, Y)

        # Backward pass and optimize
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

    validation_accuracy = model_accuracy(model, cd.validation_loader)
    print(f'Validation Accuracy after epoch [{epoch+1}/{cd.num_epochs}]: {validation_accuracy*100:.4f}%')
    
    model.eval()  # Set the model to evaluation mode
    total_loss = 0
    losses = []
    with torch.no_grad():  # Disable gradient calculations
        # calculate the validation loss
        for batch in cd.validation_loader:
            inputs, Y = batch
            inputs, Y = inputs.to(cd.device), Y.to(cd.device)

            Y = Y.view(-1, 1).float() # Reshape Y to match the shape of scores, create a column vector
            outputs = model(inputs)
            loss = criterion(outputs, Y)
            losses.append(loss.item())

    validation_loss = np.mean(np.asarray(losses)) # Calculate the average loss
    print(f'Validation Loss: {validation_loss}')


    # EARLY STOPPING
    # If the validation loss does not monotonicaly decrease, stop training
    if validation_loss < best_loss:
        best_loss = validation_loss
        epochs_without_improvement = 0
    else:
        epochs_without_improvement += 1
        if epochs_without_improvement == patience:
            print('Early stopping')
            break

# Test the model with previously unseen data
test_accuracy = model_accuracy(model, cd.test_loader)
print(f'Test Accuracy: {test_accuracy*100:.2f}%')

# Save the model
model_directory = os.path.join(cd.directory, "model256_v2.pth")
torch.save(model.state_dict(), model_directory)
