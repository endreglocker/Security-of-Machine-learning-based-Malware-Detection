import torch
import torch.nn as nn  
import torch.optim as optim  
import os
import custom_dataset as cd
from custom_models import Net

# calculate the accuracy of the model
def model_accuracy(model, loader):
    model.eval()  # Set the model to evaluation mode
    correct_predictions = 0
    total_predictions = 0
    with torch.no_grad():  # Disable gradient calculations
        for batch in loader:
            inputs, targets = batch
            outputs = model(inputs)
            targets = targets.view(-1, 1).float()
            # Apply sigmoid function to get probabilities
            probs = torch.sigmoid(outputs)

            # Convert probabilities to class predictions
            preds = (probs >= 0.5).float()

            # Compare predictions to actual targets
            correct_predictions += (preds == targets).sum().item()
            total_predictions += targets.numel()

    # Calculate accuracy
    accuracy = correct_predictions / total_predictions
    return accuracy

# Model
model = Net()
model.to(cd.device)  # Move model to GPU if available

# Define the loss function and optimizer
criterion = nn.BCEWithLogitsLoss()
optimizer = optim.Adam(model.parameters(), lr=cd.learning_rate, weight_decay=cd.weight_decay)

patience = 1  # Number of epochs to wait for improvement before stopping
best_loss = float('inf')  # Set default loss to infinity
epochs_without_improvement = 0  # Number of epochs without improvement

# Train the model
for epoch in range(cd.num_epochs):
    model.train()
    for data, targets in cd.train_loader:
        # Move data to device
        data = data.to(device=cd.device)
        targets = targets.to(device=cd.device)

        # Forward pass
        scores = model(data)
        targets = targets.view(-1, 1).float()
        loss = criterion(scores, targets)

        # Backward pass and optimize
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

    model.eval()  # Set the model to evaluation mode
    total_loss = 0
    with torch.no_grad():  # Disable gradient calculations
        # calculate the validation loss
        for batch in cd.validation_loader:
            inputs, targets = batch
            targets = targets.view(-1, 1).float()
            outputs = model(inputs)
            loss = criterion(outputs, targets)
            total_loss += loss.item()

    # Calculate the average loss over all batches
    validation_loss = total_loss / len(cd.validation_loader)
    
    print(f'Epoch [{epoch+1}/{cd.num_epochs}], Training Loss: {loss.item()}, Validation Loss: {validation_loss}')

    validation_accuracy = model_accuracy(model, cd.validation_loader)
    print(f'Validation Accuracy after epoch {epoch+1}: {validation_accuracy*100:.4f}%')

    # Check for improvement
    if validation_loss < best_loss:
        best_loss = validation_loss
        epochs_without_improvement = 0
    else:
        epochs_without_improvement += 1
        if epochs_without_improvement == patience:
            print('Early stopping')
            break


test_accuracy = model_accuracy(model, cd.test_loader)
print(f'Test Accuracy: {test_accuracy*100:.2f}%')

model_directory = os.path.join(cd.directory, "model.pth")

torch.save(model.state_dict(), model_directory)
