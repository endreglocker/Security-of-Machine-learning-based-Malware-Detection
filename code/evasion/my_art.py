import torch.nn as nn
import torch.optim as optim
import os
import torch
from art.attacks.evasion import FastGradientMethod, CarliniLInfMethod, CarliniL2Method, CarliniWagnerASR
from art.estimators.classification import PyTorchClassifier
import numpy as np

import sys
sys.path.append(os.path.join(os.getcwd(), "code", "model"))

from custom_models import LeNet5
import custom_dataset as cd



root = os.getcwd()
directory = os.path.join(root, "code", "model",'model.pth')

loading = torch.load(directory, map_location=torch.device('cpu'))

model = LeNet5()

model.load_state_dict(loading)
model.eval()


# Define the loss function and the optimizer
criterion = nn.BCELoss()
optimizer = optim.Adam(model.parameters(), lr=cd.learning_rate, weight_decay=cd.weight_decay)

# Create a PyTorchClassifier which is needed for the CarliniLInfMethod attack
classifier = PyTorchClassifier(
    model=model,
    clip_values=(0, 1),
    loss=criterion,
    optimizer=optimizer,
    input_shape=(1, 256, 256),  # Adjust this to match the input shape of your model
    nb_classes=cd.num_classes,
)

# Create the CarliniLInfMethod attack
attack = FastGradientMethod(classifier)

x_test = cd.test_loader

x_test_array = next(iter(x_test))[0].numpy()

x_test, y_test = next(iter(cd.test_loader))

#print(x_test.shape)
mask = np.zeros((256, 256)).astype(float)
mask[:60, :60] = 1.0
x_test_masked = (x_test.numpy() * mask).astype(float)

model = model.float()

x_test_masked = x_test_masked.astype(np.float32)
x_test_adv = attack.generate(x=x_test_masked)

x_test_adv = torch.from_numpy(x_test_adv).float().to(cd.device)

model.eval()
with torch.no_grad():
    outputs = model(x_test_adv)
    predicted = torch.round(outputs).int()

for i in range(len(predicted)):
    print(f"P = {predicted[i].item()} | GT = {y_test[i]}")

correct = (predicted == y_test.view(-1, 1).to(cd.device)).sum().item()
mistake = (predicted != y_test.view(-1, 1).to(cd.device)).sum().item()
number_of_malware = (y_test == 1).sum().item()

accuracy = correct / len(y_test)

print(f'Accuracy on adversarial examples: {accuracy * 100}%')

print(f'Misidentified malware: {mistake} out of {number_of_malware} ({mistake / number_of_malware * 100}%)')

def model_accuracy(model, loader):
    model.eval()
    correct = 0
    total = 0
    with torch.no_grad():
        for data, labels in loader:
            data = data.to(device=cd.device)
            labels = labels.view(-1, 1).to(device=cd.device)  # Reshape the labels tensor
            outputs = model(data)
            predicted = torch.round(outputs)  # Round the output probabilities to get the predicted classes
            total += labels.size(0)
            correct += (predicted == labels).sum().item()
    return correct / total

test_accuracy = model_accuracy(model, cd.test_loader)
print(f'Test Accuracy: {test_accuracy*100:.2f}%')