import os
import torch
import attack_implementations as aim
import torch.nn as nn
import sys
sys.path.append(os.path.join(os.getcwd(), "code", "model"))
from custom_models import Net

sys.path.append(os.path.join(os.getcwd(), "code"))
import load_data as ld

# Load the model & set it to evaluation mode
directory = os.path.join(os.getcwd(), "code", "model",'model256_v2.pth')

loading = torch.load(directory, map_location=torch.device('cpu'))

model = Net()

model.load_state_dict(loading)
model.eval()

# Load the datasets
file_percentage = 0.095
malwares, benigns = ld.load_file_names()
data_array, ground_truth, original_length, bigger_or_equal = ld.file_to_array(malwares[100], percentage=file_percentage)
data_array = data_array.unsqueeze(0)

data_array = data_array.to(next(model.parameters()).device)
data_array.requires_grad = True

# Set the hyperparameters for the attack
epsilon = 0.5
alpha = 1e-2
num_iter = 40
restarts = 20
loss_fn = nn.BCEWithLogitsLoss()

# Calculate the percentage scaling factor if the file is compressed
model_input_data_length = 256*256
modified_percentage = file_percentage * model_input_data_length / (original_length * (1 + file_percentage)) 

percentage = modified_percentage if bigger_or_equal else file_percentage
print(f"Percentage: {percentage}")
# Attack the model
delta = aim.pgd_linf_rand(model, data_array, ground_truth, epsilon, alpha, num_iter, restarts, loss_fn, percentage)

# Evaluate the original data
data_prediction = model(data_array)
data_prediction.backward()

# Create the perturbated data
perturbated_image = data_array + delta
perturbated_image.clamp_(0, 1)

aim.save_tensors("test2",delta, data_array, perturbated_image)

perturbated_image_leaf = perturbated_image.detach().clone()
perturbated_image_leaf.requires_grad = True

perturbated_pred = model(perturbated_image_leaf)
perturbated_pred.backward()

dict = {0: "benign", 1: "malware"}
sol_o = torch.sigmoid(data_prediction).round().int().item()
sol_p = torch.sigmoid(perturbated_pred).round().int().item()

print(f"Original:\t{dict[sol_o]}")
print(f"Perturbated:\t{dict[sol_p]}")

aim.plot_images(data_array, perturbated_image_leaf, delta, dict, dict[sol_o], dict[sol_p])


# vissza byte arrajbe

# suffex / teljes fájl arány legyen két három teszt pl. max 10% + vissza mappelés
# ellenőrzés, hogy a kép megfelelő-e

# robosztusság

# megtámadott mintán frissíteni a modellt
# megnézni, hogy mennyire javult a modell, ha tanító mintán frissítjük

# black box -> vagy transfer learning / approximálni a gradienst
# határértékes differenciálás def alapján

# poisoning - tanító minta manipulálása
# poising ellentétes grádienssel beszúrni -> semlegesíti / felerősíte egyes feature-öket

# gradient alignment