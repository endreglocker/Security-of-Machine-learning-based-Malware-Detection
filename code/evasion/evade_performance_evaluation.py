import os
import torch
import attack_implementations as aim
import torch.nn as nn
import sys
sys.path.append(os.path.join(os.getcwd(), "code", "model"))
from custom_models import Net

sys.path.append(os.path.join(os.getcwd(), "code"))
import load_data as ld

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

# Load the model & set it to evaluation mode
directory = os.path.join(os.getcwd(), "code", "model",'model256_v2.pth')

loading = torch.load(directory, map_location=device)

model = Net()

model.load_state_dict(loading)
model.eval()

# Load the datasets
malwares, benigns = ld.load_file_names()

def performance(dataset, file_percentage=0.1):
    algorithm_correct = 0
    suffix_correct = 0
    i = 0

    total = len(dataset)
    # Set the hyperparameters for the attack
    epsilon = 1
    alpha = 1e-2
    num_iter = 40
    restarts = 20
    loss_fn = nn.BCEWithLogitsLoss()
    model_input_data_length = 65536 #256*256
    for d in dataset:
        data_array, ground_truth, original_length, bigger_or_equal = ld.file_to_array(d, percentage=file_percentage)
        data_array = data_array.unsqueeze(0)

        data_array = data_array.to(next(model.parameters()).device)

        # Calculate the percentage scaling factor if the file is compressed
        rate_of_compression = original_length / model_input_data_length
        modified_percentage = file_percentage / (1 + file_percentage)
        
        targeted_percentage = modified_percentage if bigger_or_equal else (modified_percentage * rate_of_compression)

        # Perform the attack
        delta = aim.pgd_linf_rand(model, data_array, ground_truth, epsilon, alpha, num_iter, restarts, loss_fn, targeted_percentage)
        delta_copy = delta.detach().clone()
        delta_copy.clamp_(0,1)
        
        perturbated_image = data_array + delta
        perturbated_image.clamp_(0, 1)

        perturbated_image_leaf = perturbated_image.detach().clone()

        perturbated_pred = model(perturbated_image_leaf)

        ground_truth_value = ground_truth.item()
        prediction_value = torch.sigmoid(perturbated_pred).round().item()
        print(f"{i}/{total}\nGround Truth: {ground_truth_value}\tPrediction: {prediction_value}")
        i += 1
        if ground_truth_value == prediction_value:
            algorithm_correct += 1
        
        # get the suffix
        original_file_length_with_suffix = int(original_length * (1 + file_percentage))
        suffix_as_bytes = ld.suffix_to_bytes(delta_copy, original_file_length_with_suffix, bigger_or_equal, targeted_percentage, modified_percentage)
        # read file
        read_original_file = ld.read_file(d)
        # add suffix to the original file
        read_original_file.extend(suffix_as_bytes)
        # tranform the file to model input
        resized_orginal_file, _ = ld.resize_array(read_original_file)
        with_suffix_to_tensor = ld.transform_to_tensor(resized_orginal_file)
        with_suffix_to_tensor.unsqueeze_(0)
        with_suffix_to_tensor = with_suffix_to_tensor.to(next(model.parameters()).device)
        # evaluate the file with the added suffix
        with_suffix_eval = model(with_suffix_to_tensor)
        with_suffix_prediction = torch.sigmoid(with_suffix_eval).round().item()
        
        if ground_truth_value == with_suffix_prediction:
            suffix_correct += 1
        
    return algorithm_correct / total, suffix_correct / total


path = os.path.join(os.getcwd(), "raw", "evasion_performance")

with open(path, 'a') as f:
    f.write('Malware Evasion Performance  / with epsilon = 1.0 / only using PGD function and not directly adding data to the original malware\n')
    f.write('---------------------------\n')

    acc, sfx5 = performance(malwares, 0.05)
    print(f"Malware evasion accuracy at 5% modification:\t\t{acc} = {acc*100}%")
    f.write(f"Malware evasion accuracy at 5% modification:\t\t{acc} = {acc*100}%\n")
    
    acc, sfx10 = performance(malwares, 0.1)
    print(f"Malware evasion accuracy at 10% modification:\t\t{acc} = {acc*100}%")
    f.write(f"Malware evasion accuracy at 10% modification:\t\t{acc} = {acc*100}%\n")

    acc, sfx15 = performance(malwares, 0.15)
    print(f"Malware evasion accuracy at 15% modification:\t\t{acc} = {acc*100}%")
    f.write(f"Malware evasion accuracy at 15% modification:\t\t{acc} = {acc*100}%\n")

    acc, sfx20 = performance(malwares, 0.2)
    print(f"Malware evasion accuracy at 20% modification:\t\t{acc} = {acc*100}%")
    f.write(f"Malware evasion accuracy at 20% modification:\t\t{acc} = {acc*100}%\n\n")
    
    f.write('Malware Evasion Performance  / with epsilon = 1.0 / cutting perturbated part, then concatenate it as a suffix to the original\n')
    f.write('---------------------------\n')

    print(f"Malware evasion accuracy at 5% modification:\t\t{sfx5} = {sfx5*100}%")
    f.write(f"Malware evasion accuracy at 5% modification:\t\t{sfx5} = {sfx5*100}%\n")
    
    print(f"Malware evasion accuracy at 10% modification:\t\t{sfx10} = {sfx10*100}%")
    f.write(f"Malware evasion accuracy at 10% modification:\t\t{sfx10} = {sfx10*100}%\n")

    print(f"Malware evasion accuracy at 15% modification:\t\t{sfx15} = {sfx15*100}%")
    f.write(f"Malware evasion accuracy at 15% modification:\t\t{sfx15} = {sfx15*100}%\n")

    print(f"Malware evasion accuracy at 20% modification:\t\t{sfx20} = {sfx20*100}%")
    f.write(f"Malware evasion accuracy at 20% modification:\t\t{sfx20} = {sfx20*100}%\n")
