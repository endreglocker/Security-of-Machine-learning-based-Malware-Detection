import os
import torch
import numpy as np
import math

def load_file_names():
    malware_root_path = os.path.join(os.getcwd(), "code", "dataset","malware")
    benign_root_path = os.path.join(os.getcwd(), "code", "dataset","benign")
    m_arm = os.path.join(malware_root_path, "arm")
    m_mips = os.path.join(malware_root_path, "mips")
    b_arm = os.path.join(benign_root_path, "arm")
    b_mips = os.path.join(benign_root_path, "mips")

    malwares = []
    for filename in os.listdir(m_arm):
        image_path = os.path.join(m_arm, filename)
        data = (image_path,1)
        malwares.append(data)

    for filename in os.listdir(m_mips):
        image_path = os.path.join(m_mips, filename)
        data = (image_path,1)
        malwares.append(data)

    benigns = []
    for filename in os.listdir(b_arm):
        image_path = os.path.join(b_arm, filename)
        data = (image_path,0)
        benigns.append(data)

    for filename in os.listdir(b_mips):
        image_path = os.path.join(b_mips, filename)
        data = (image_path,0)
        benigns.append(data)

    return malwares, benigns

def read_file(path):
    with open(path[0], 'rb') as f:
        binary_data = f.read()

    binary_data = [int(b) for b in binary_data]  # Convert bytes to integers
    return binary_data

def transform_to_tensor(binary_data):
    binary_data = [x / 255.0 for x in binary_data]
    # Convert the list to a tensor
    tensor_data = torch.tensor(binary_data, dtype=torch.float32)

    # Reshape the tensor to the expected input shape
    tensor_data = tensor_data.view(1, -1).float()
    return tensor_data

def file_to_array(path, target_length=256*256, percentage=0.0):
    binary_data = read_file(path)
    original_length = len(binary_data)
    # Resize the data to the target length
    binary_data, bigger_or_equal = resize_array(binary_data, target_length, percentage)

    tensor_data = transform_to_tensor(binary_data)

    ground_truth = torch.tensor([path[1]]).view(1,1).float()

    return tensor_data, ground_truth, original_length, bigger_or_equal

def resize_array(binary_data, target_length=256*256, percentage=0.0):
    file_length = len(binary_data)
    file_buffer_length = 1.0 + percentage 
    modified_target_length = int(file_buffer_length * file_length)
    isBiggerOrEqual = False
    
    if percentage > 0.0:
        if modified_target_length <= target_length:
            isBiggerOrEqual = False
        else:
            binary_data = padding(binary_data, modified_target_length)
            isBiggerOrEqual = True
    
    modified_data = []

    if modified_target_length >= target_length:
        modified_data = shrink(binary_data, target_length)
    elif modified_target_length < target_length:
        modified_data =  padding(binary_data, target_length)
        
    return modified_data, isBiggerOrEqual

def padding(binary_data, target_length):
    return binary_data + [0] * (target_length - len(binary_data))

def shrink(binary_data, target_length):
    factor = len(binary_data) / target_length
    shrunk_array = []
    for i in range(target_length):
        start_index = int(i * factor)
        end_index = int((i + 1) * factor)
        #segment_average = int(np.mean(binary_data[start_index:end_index]).round())
        segment_average = np.mean(binary_data[start_index:end_index])
        shrunk_array.append(segment_average)
    return shrunk_array

def suffix_to_bytes(tensor_data, original_length, isBigger, targeted_percentage=0.0, original_percentage=0.0):
    tensor_data = tensor_data.view(-1)
    tensor_data = tensor_data * 255.0
    data_array = tensor_data.numpy()
    data_length = len(data_array)
    #index = int(math.ceil(targeted_percentage * data_length))
    index = int(targeted_percentage * data_length)
    
    cut = []
    
    if isBigger:
        substracted = data_array[-index:]
        upscaling_factor = int(math.ceil(original_length / data_length))
        targeted_length = int(original_length * original_percentage)
                
        cut = resize_to_target_suffix(substracted, upscaling_factor, targeted_length)
    else:
        cut = data_array[-index:]
    
    cut = [int(round(x)) for x in cut]
    return cut

def resize_to_target_suffix(data, factor, target_length):
    upscaled = upscale(data, factor)
    shrinked = shrink(upscaled, target_length)
    return shrinked

def upscale(data, factor):
    upscaled = []
    for d in data:
        for j in range(factor):
            upscaled.append(d)
    return upscaled

def downscale(data, target_length):
    factor = len(data) / target_length
    shrunk_array = []
    for i in range(target_length):
        start_index = int(i * factor)
        end_index = int((i + 1) * factor)
        segment_average = np.mean(data[start_index:end_index])
        shrunk_array.append(segment_average)
    return shrunk_array